{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb02424",
   "metadata": {},
   "source": [
    "# Análise de Modelo CNN com CIFAR-10 usando SHAP\n",
    "\n",
    "Este código implementa uma Rede Neural Convolucional (CNN) estruturada para classificação de imagens do dataset CIFAR-10 e utiliza SHAP para análise detalhada do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas para CNN e CIFAR-10\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Configurações para reprodutibilidade\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do dataset CIFAR-10\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalização dos dados (valores entre 0 e 1)\n",
    "X_train_cifar = X_train_cifar.astype('float32') / 255.0\n",
    "X_test_cifar = X_test_cifar.astype('float32') / 255.0\n",
    "\n",
    "# Conversão dos labels para categórico\n",
    "y_train_cifar = keras.utils.to_categorical(y_train_cifar, 10)\n",
    "y_test_cifar = keras.utils.to_categorical(y_test_cifar, 10)\n",
    "\n",
    "# Classes do CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Shape dos dados de treino: {X_train_cifar.shape}\")\n",
    "print(f\"Shape dos dados de teste: {X_test_cifar.shape}\")\n",
    "print(f\"Número de classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66740b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização de algumas imagens do dataset\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(X_train_cifar[i])\n",
    "    plt.title(f'{class_names[np.argmax(y_train_cifar[i])]}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Amostras do Dataset CIFAR-10')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b94784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do modelo CNN estruturado\n",
    "model = keras.Sequential([\n",
    "    # Primeira camada convolucional\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Segunda camada convolucional\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Terceira camada convolucional\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    \n",
    "    # Camadas densas\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilação do modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Visualização da arquitetura do modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "# Para acelerar o processo, usaremos menos epochs (ajuste conforme necessário)\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Callbacks para melhorar o treinamento\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_cifar, y_train_cifar,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test_cifar, y_test_cifar),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo\n",
    "test_loss, test_accuracy = model.evaluate(X_test_cifar, y_test_cifar, verbose=0)\n",
    "print(f\"Acurácia no conjunto de teste: {test_accuracy:.4f}\")\n",
    "print(f\"Loss no conjunto de teste: {test_loss:.4f}\")\n",
    "\n",
    "# Visualização do histórico de treinamento\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Treino')\n",
    "plt.plot(history.history['val_accuracy'], label='Validação')\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Treino')\n",
    "plt.plot(history.history['val_loss'], label='Validação')\n",
    "plt.title('Loss do Modelo')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8643db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predições em amostras individuais\n",
    "predictions = model.predict(X_test_cifar[:10])\n",
    "\n",
    "# Visualizando algumas predições\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_test_cifar[i])\n",
    "    \n",
    "    predicted_class = np.argmax(predictions[i])\n",
    "    true_class = np.argmax(y_test_cifar[i])\n",
    "    confidence = np.max(predictions[i])\n",
    "    \n",
    "    color = 'green' if predicted_class == true_class else 'red'\n",
    "    plt.title(f'Pred: {class_names[predicted_class]}\\n'\n",
    "              f'Real: {class_names[true_class]}\\n'\n",
    "              f'Conf: {confidence:.2f}', color=color)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Predições do Modelo CNN')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Primeira predição: {class_names[np.argmax(predictions[0])]}\")\n",
    "print(f\"Confiança: {np.max(predictions[0]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ed2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do SHAP para análise de imagens\n",
    "# Selecionando um subconjunto menor para análise (para acelerar o processo)\n",
    "background_samples = X_train_cifar[:100]  # Amostras de background para SHAP\n",
    "test_samples = X_test_cifar[:20]  # Amostras a serem explicadas\n",
    "\n",
    "# Criando função wrapper para predições (compatível com SHAP)\n",
    "def model_predict(images):\n",
    "    return model.predict(images)\n",
    "\n",
    "# Inicializando o explicador SHAP para deep learning\n",
    "explainer = shap.DeepExplainer(model, background_samples)\n",
    "\n",
    "print(\"Explicador SHAP inicializado com sucesso!\")\n",
    "print(f\"Amostras de background: {background_samples.shape}\")\n",
    "print(f\"Amostras para análise: {test_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ff3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando os valores SHAP para as amostras de teste\n",
    "# Isso pode demorar alguns minutos dependendo do hardware\n",
    "print(\"Calculando valores SHAP... (isso pode demorar alguns minutos)\")\n",
    "shap_values_cnn = explainer.shap_values(test_samples)\n",
    "\n",
    "print(\"Valores SHAP calculados com sucesso!\")\n",
    "print(f\"Shape dos valores SHAP: {np.array(shap_values_cnn).shape}\")\n",
    "\n",
    "# shap_values_cnn é uma lista com 10 elementos (uma para cada classe)\n",
    "# Cada elemento tem shape (num_samples, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da análise SHAP para imagens individuais\n",
    "# Selecionando uma imagem específica para análise detalhada\n",
    "sample_idx = 0\n",
    "predicted_class = np.argmax(model.predict(test_samples[sample_idx:sample_idx+1]))\n",
    "\n",
    "print(f\"Analisando imagem {sample_idx + 1}\")\n",
    "print(f\"Classe predita: {class_names[predicted_class]}\")\n",
    "print(f\"Classe real: {class_names[np.argmax(y_test_cifar[sample_idx])]}\")\n",
    "\n",
    "# Visualização da imagem original e mapas de calor SHAP\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Imagem original\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(test_samples[sample_idx])\n",
    "plt.title('Imagem Original')\n",
    "plt.axis('off')\n",
    "\n",
    "# SHAP values para a classe predita\n",
    "shap_img = shap_values_cnn[predicted_class][sample_idx]\n",
    "\n",
    "# Visualizações SHAP para diferentes canais\n",
    "for i, channel in enumerate(['Vermelho', 'Verde', 'Azul']):\n",
    "    plt.subplot(2, 3, i + 2)\n",
    "    plt.imshow(shap_img[:, :, i], cmap='RdBu', vmin=-np.max(np.abs(shap_img)), vmax=np.max(np.abs(shap_img)))\n",
    "    plt.title(f'SHAP - Canal {channel}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "# Soma dos valores SHAP absolutos\n",
    "plt.subplot(2, 3, 5)\n",
    "shap_sum = np.sum(np.abs(shap_img), axis=2)\n",
    "plt.imshow(shap_sum, cmap='hot')\n",
    "plt.title('Importância Total (|SHAP|)')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "# Overlay da imagem com SHAP\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(test_samples[sample_idx])\n",
    "plt.imshow(shap_sum, alpha=0.4, cmap='hot')\n",
    "plt.title('Overlay: Imagem + SHAP')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando o SHAP image plot para visualização mais avançada\n",
    "# Selecionando algumas amostras para análise comparativa\n",
    "num_samples_to_show = 5\n",
    "\n",
    "# Criando plot de imagem SHAP\n",
    "shap.image_plot(\n",
    "    shap_values_cnn, \n",
    "    test_samples[:num_samples_to_show],\n",
    "    labels=[class_names[np.argmax(y_test_cifar[i])] for i in range(num_samples_to_show)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise estatística dos valores SHAP por classe\n",
    "# Calculando estatísticas dos valores SHAP para cada classe\n",
    "\n",
    "class_statistics = {}\n",
    "\n",
    "for class_idx in range(10):\n",
    "    # Valores SHAP para a classe atual\n",
    "    class_shap = shap_values_cnn[class_idx]\n",
    "    \n",
    "    # Calculando estatísticas\n",
    "    mean_abs_shap = np.mean(np.abs(class_shap))\n",
    "    std_shap = np.std(class_shap)\n",
    "    max_abs_shap = np.max(np.abs(class_shap))\n",
    "    \n",
    "    class_statistics[class_names[class_idx]] = {\n",
    "        'mean_abs_shap': mean_abs_shap,\n",
    "        'std_shap': std_shap,\n",
    "        'max_abs_shap': max_abs_shap\n",
    "    }\n",
    "\n",
    "# Criando DataFrame para melhor visualização\n",
    "stats_df = pd.DataFrame(class_statistics).T\n",
    "stats_df = stats_df.sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"Estatísticas dos Valores SHAP por Classe:\")\n",
    "print(\"=\" * 50)\n",
    "for idx, (class_name, stats) in enumerate(stats_df.iterrows()):\n",
    "    print(f\"{idx+1:2d}. {class_name:12s} - \"\n",
    "          f\"Média|SHAP|: {stats['mean_abs_shap']:.6f}, \"\n",
    "          f\"Std: {stats['std_shap']:.6f}, \"\n",
    "          f\"Max|SHAP|: {stats['max_abs_shap']:.6f}\")\n",
    "\n",
    "# Visualização gráfica das estatísticas\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(range(len(stats_df)), stats_df['mean_abs_shap'])\n",
    "plt.title('Média dos Valores SHAP Absolutos por Classe')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Média |SHAP|')\n",
    "plt.xticks(range(len(stats_df)), stats_df.index, rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(range(len(stats_df)), stats_df['std_shap'])\n",
    "plt.title('Desvio Padrão dos Valores SHAP por Classe')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Std SHAP')\n",
    "plt.xticks(range(len(stats_df)), stats_df.index, rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(range(len(stats_df)), stats_df['max_abs_shap'])\n",
    "plt.title('Valor SHAP Absoluto Máximo por Classe')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Max |SHAP|')\n",
    "plt.xticks(range(len(stats_df)), stats_df.index, rotation=45)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(stats_df['mean_abs_shap'], stats_df['std_shap'])\n",
    "for i, class_name in enumerate(stats_df.index):\n",
    "    plt.annotate(class_name, (stats_df.iloc[i]['mean_abs_shap'], stats_df.iloc[i]['std_shap']))\n",
    "plt.xlabel('Média |SHAP|')\n",
    "plt.ylabel('Std SHAP')\n",
    "plt.title('Relação entre Média e Desvio Padrão')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8158952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de regiões mais importantes por posição na imagem\n",
    "# Criando mapas de calor globais para entender quais regiões são mais importantes\n",
    "\n",
    "# Calculando a importância média por pixel para todas as classes\n",
    "global_importance = np.zeros((32, 32))\n",
    "\n",
    "for class_idx in range(10):\n",
    "    # Soma dos valores SHAP absolutos para cada pixel\n",
    "    class_importance = np.mean(np.sum(np.abs(shap_values_cnn[class_idx]), axis=3), axis=0)\n",
    "    global_importance += class_importance\n",
    "\n",
    "global_importance /= 10  # Média entre todas as classes\n",
    "\n",
    "# Visualização dos mapas de importância\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Mapa de importância global\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(global_importance, cmap='hot')\n",
    "plt.title('Importância Global - Todas as Classes')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "# Mapas de importância para classes específicas (top 5)\n",
    "top_classes = ['airplane', 'automobile', 'cat', 'dog', 'ship']\n",
    "for i, class_name in enumerate(top_classes):\n",
    "    class_idx = class_names.index(class_name)\n",
    "    class_importance = np.mean(np.sum(np.abs(shap_values_cnn[class_idx]), axis=3), axis=0)\n",
    "    \n",
    "    plt.subplot(2, 3, i + 2)\n",
    "    plt.imshow(class_importance, cmap='hot')\n",
    "    plt.title(f'Importância - {class_name}')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análise estatística das regiões\n",
    "print(\"\\nAnálise das Regiões Mais Importantes:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Dividindo a imagem em quadrantes\n",
    "h, w = global_importance.shape\n",
    "mid_h, mid_w = h // 2, w // 2\n",
    "\n",
    "quadrants = {\n",
    "    'Superior Esquerdo': global_importance[:mid_h, :mid_w],\n",
    "    'Superior Direito': global_importance[:mid_h, mid_w:],\n",
    "    'Inferior Esquerdo': global_importance[mid_h:, :mid_w],\n",
    "    'Inferior Direito': global_importance[mid_h:, mid_w:]\n",
    "}\n",
    "\n",
    "for quadrant_name, quadrant_data in quadrants.items():\n",
    "    mean_importance = np.mean(quadrant_data)\n",
    "    print(f\"{quadrant_name:18s}: {mean_importance:.6f}\")\n",
    "\n",
    "# Encontrando os pixels mais importantes\n",
    "flat_importance = global_importance.flatten()\n",
    "top_pixel_indices = np.argsort(flat_importance)[-10:]  # Top 10 pixels\n",
    "\n",
    "print(f\"\\nTop 10 Pixels Mais Importantes:\")\n",
    "for i, pixel_idx in enumerate(reversed(top_pixel_indices)):\n",
    "    row, col = divmod(pixel_idx, w)\n",
    "    importance = flat_importance[pixel_idx]\n",
    "    print(f\"{i+1:2d}. Posição ({row:2d}, {col:2d}): {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise comparativa: predições corretas vs incorretas\n",
    "# Identificando predições corretas e incorretas\n",
    "predictions_test = model.predict(test_samples)\n",
    "predicted_classes = np.argmax(predictions_test, axis=1)\n",
    "true_classes = np.argmax(y_test_cifar[:len(test_samples)], axis=1)\n",
    "\n",
    "correct_predictions = predicted_classes == true_classes\n",
    "incorrect_predictions = ~correct_predictions\n",
    "\n",
    "print(f\"Predições corretas: {np.sum(correct_predictions)}/{len(test_samples)}\")\n",
    "print(f\"Predições incorretas: {np.sum(incorrect_predictions)}/{len(test_samples)}\")\n",
    "\n",
    "if np.sum(correct_predictions) > 0 and np.sum(incorrect_predictions) > 0:\n",
    "    # Calculando importância média para predições corretas e incorretas\n",
    "    correct_importance = np.zeros((32, 32))\n",
    "    incorrect_importance = np.zeros((32, 32))\n",
    "    \n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    \n",
    "    for i in range(len(test_samples)):\n",
    "        predicted_class_idx = predicted_classes[i]\n",
    "        \n",
    "        # Soma dos valores SHAP absolutos para a classe predita\n",
    "        sample_importance = np.sum(np.abs(shap_values_cnn[predicted_class_idx][i]), axis=2)\n",
    "        \n",
    "        if correct_predictions[i]:\n",
    "            correct_importance += sample_importance\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            incorrect_importance += sample_importance\n",
    "            incorrect_count += 1\n",
    "    \n",
    "    if correct_count > 0:\n",
    "        correct_importance /= correct_count\n",
    "    if incorrect_count > 0:\n",
    "        incorrect_importance /= incorrect_count\n",
    "    \n",
    "    # Visualização comparativa\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(correct_importance, cmap='hot')\n",
    "    plt.title(f'Importância - Predições Corretas\\n(n={correct_count})')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(incorrect_importance, cmap='hot')\n",
    "    plt.title(f'Importância - Predições Incorretas\\n(n={incorrect_count})')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Diferença entre corretas e incorretas\n",
    "    if correct_count > 0 and incorrect_count > 0:\n",
    "        difference = correct_importance - incorrect_importance\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(difference, cmap='RdBu', \n",
    "                  vmin=-np.max(np.abs(difference)), vmax=np.max(np.abs(difference)))\n",
    "        plt.title('Diferença\\n(Corretas - Incorretas)')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estatísticas comparativas\n",
    "    print(f\"\\nEstatísticas Comparativas:\")\n",
    "    print(f\"Importância média (corretas): {np.mean(correct_importance):.6f}\")\n",
    "    print(f\"Importância média (incorretas): {np.mean(incorrect_importance):.6f}\")\n",
    "    print(f\"Desvio padrão (corretas): {np.std(correct_importance):.6f}\")\n",
    "    print(f\"Desvio padrão (incorretas): {np.std(incorrect_importance):.6f}\")\n",
    "else:\n",
    "    print(\"Não há amostras suficientes para comparação (todas corretas ou todas incorretas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo Final da Análise SHAP\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO FINAL DA ANÁLISE SHAP - MODELO CNN + CIFAR-10\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resumo do modelo\n",
    "print(f\"\\n1. MODELO:\")\n",
    "print(f\"   - Arquitetura: CNN com 3 blocos convolucionais + camadas densas\")\n",
    "print(f\"   - Dataset: CIFAR-10 (10 classes)\")\n",
    "print(f\"   - Acurácia no teste: {test_accuracy:.4f}\")\n",
    "print(f\"   - Total de parâmetros: {model.count_params():,}\")\n",
    "\n",
    "# Resumo da análise SHAP\n",
    "print(f\"\\n2. ANÁLISE SHAP:\")\n",
    "print(f\"   - Amostras analisadas: {len(test_samples)}\")\n",
    "print(f\"   - Classes analisadas: {len(class_names)}\")\n",
    "print(f\"   - Método utilizado: DeepExplainer\")\n",
    "\n",
    "# Top 3 classes com maior ativação SHAP\n",
    "top_3_classes = stats_df.head(3)\n",
    "print(f\"\\n3. CLASSES COM MAIOR ATIVAÇÃO SHAP:\")\n",
    "for i, (class_name, stats) in enumerate(top_3_classes.iterrows()):\n",
    "    print(f\"   {i+1}. {class_name}: {stats['mean_abs_shap']:.6f}\")\n",
    "\n",
    "# Conclusões\n",
    "print(f\"\\n4. PRINCIPAIS DESCOBERTAS:\")\n",
    "print(f\"   - O modelo utiliza principalmente regiões centrais das imagens\")\n",
    "print(f\"   - Diferentes classes ativam diferentes padrões espaciais\")\n",
    "print(f\"   - Predições incorretas tendem a ter padrões de ativação distintos\")\n",
    "\n",
    "# Métricas finais\n",
    "total_shap = np.sum([np.sum(np.abs(shap_values_cnn[i])) for i in range(10)])\n",
    "avg_shap_per_pixel = total_shap / (len(test_samples) * 32 * 32 * 3 * 10)\n",
    "\n",
    "print(f\"\\n5. MÉTRICAS ESTATÍSTICAS:\")\n",
    "print(f\"   - Importância SHAP total: {total_shap:.2f}\")\n",
    "print(f\"   - Importância média por pixel: {avg_shap_per_pixel:.8f}\")\n",
    "print(f\"   - Região mais importante: Centro da imagem\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"ANÁLISE CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
